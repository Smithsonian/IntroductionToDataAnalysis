---
title: "An Introduction to Data Analysis in R"
author: "Smithsonian's National Zoo & Conservation Biology Institute"
date: '`r format(Sys.time(), "%d %B %Y")`'
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: 
      collapsed: true
      smooth_scroll: true
    number_sections: false
pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction to Data Analysis

Now that you've learned the basics of R programming, we'll take things a step further.  In this chapter, we will walk you through a new set of analyses.

We'll be working with the dataset "mtcars" which comes pre-loaded with R. The goal of this exercise is to test your basic skills in R programming, specifically in manipulating data and to reiterate some principles in statistical modelling.  This chapter will help as we move towards our ultimate goal of conducting more advanced analyses.

You may not be familiar with all the operations you need to execute in this exercise. Part of the goal with this exercise, however, is for you to become more familiar with the *help* commands in R and with the internet solutions that exist.  Our ultimate goal is to make you aware of the tools that are available so that you can become an effective problem solver, working independently on data analyses.

Whenever you start working with a new script, you should first set a working directory. If you are working within a **R-Studio** project, the working directory will automatically be set by default.  This directory will contain all the data for your analysis and will be where you will save all the data outputs. 

Now let's change the working directory to a location of your choosing. Create a folder if you don't have one already, then make sure your working directory is in that folder. If you already have a folder, just set the working directory to the folder you want to use.

# Clearing the Environment

You'll find that your Environment (Workspace) in the upper right panel will quickly become cluttered with user-defined objects.  It's generally good practice to work with a clean Workspace when starting a session.  You can also remove individual files should you need to do so (`rm(dataset)`).  I generaly start all my scripts with the following command to make sure you are starting fresh, something we will do to help develop good good programming practices.

```{r, eval=F}
# Clean your workspace/remove all objects
rm(list=ls())

# Remove an individual dataset
rm(dataset)
```


## Statistical Modelling
### General Principles

In all statistical models, you specify a **dependent variable** (the thing you are trying to explain) and one or more **independent variables** (the thing or several things that you are using to explain the dependent variable).

To build a model you will first need to decide what dependent and independent variables to include. That depends on your research question. Then you need to decide what kind of model structure is appropriate for the data set. 

Before you interpret or use a model at all, you need to check that the assumptions of the model are valid **(model validation)**, decide what is the best model structure **(model selection)** and then, finally, **interpret the model and make predictions.**

The general structure for defining any model in **R** is:

**model.type(dependent.variable ~ independent.variable1 + independent.variable2, data = dataframe)**

Note that you do not have to specify the intercept in the model equation. We assume there are 2 different independent variables in the example provided (independent.variable1 and independent.variable2), but the model could contain any number of independent variables.

### Regression vs Classification

The dependent variable can either be continuous or categorical data. We refer to these separate parameterizations as a regression model or a classification model, respectively. Most of the models we demonstrate here are regression models, but the structure to build classification models in **R** are almost identical. In most cases, the data type of the dependent variable determines whether the model is a regression (non-categorical numeric-type dependent variable) or classification (categorical factor-type dependent variable). You can use functions such as `as.factor()` and `as.numeric()` to convert different data types of data to a factor or numeric variable. Note, just because a data set contains numbers, does **NOT** necessarily mean the numbers are numeric-type. Numbers can be used as symbols to differentiate categories as well. It is always a good practice to confirm the data type of the dependent and independent variables that you are inputing into the model.   

### Exploring the Data

Let's start investigating a data set to later fit a linear model.

Load the "mtcars"" data set.

```{r}
data(mtcars)
```

View the first 10 lines of the data set.

```{r}
head(mtcars, 10)
```

Assess the overall structure of the data set to get a sense of the number and type of variables included. When you work with your own data, you will be familiar with the data structure, but it is always good practice to examine your data before moving on to any model fitting. Assure that the data structure of each column of the data frame is correct and/or what you expect it to be. 

Note, all columns/variables included in this sample dataset are numeric-type.  You can confirm the data type of each column by typing **is.numeric()** next to the variable name (e.g., `is.numeric(mtcars$mpg)`). 

```{r}
str(mtcars)
```

Now summarize the data to provide a list of each variable with the mean, min, and max.

```{r}
summary(mtcars)
```

'Summary' is a great function to have a quick view at the data. But what if you want to save the mean, min, or max values of each variable? There is a family of functions in **R** that are great for applying functions to all columns or all rows in a matrix and that return the result as a vector or list of values. This is the `apply` function. 

The `apply` function has two main arguments. The MARGIN that is a 1 or a 2, indicating whether you want to operate on rows (1) or columns (2) and the FUN arguments that tell **R** what function is to be applied. For example, to obtain the mean of each variable in the mtcars dataset, we do:

```{r}
apply(mtcars, 2, mean)
```
Compare with the values reported by the `summary` function.

> QUESTION: Can you calculate the min and max values for each variable?

Since this is an exercise involving regression, let's have a quick look at all our variables together (since they are all numeric) by looking at scatter plots of each variable combination. Use the function "pairs" or "plot" on the data set.

```{r}
plot(mtcars)
```

You should be able to see cases where there seems to be a strong relationship between two variables. "mpg" vs. "wt" is a good example of this. This is miles per gallon vs. the weight of the car, and this makes sense. Is the slope here positive or negative?

We can plot these two variables against each other to examine the relationship closer. 

```{r}
plot(mtcars$mpg ~ mtcars$wt)
```

You could plot any of the variables in the data frame. Plotting the data is one of the simplest ways to look and explore the data.

In **R** you can customize your plots. Can you change the 'x' and 'y' variable names and add a title to the plot?  Use the help file `help(plot)` or `?plot` to determine the proper syntax, or simply Google "add tile to plot in R".

```{r}
plot(mtcars$mpg ~ mtcars$wt, xlab="Weight", ylab="MPG", main="MPG vs Weight")
```

Calculate the correlation coefficient between the two variables, then perform a correlation test to see if they are significantly correlated.

```{r}
cor(mtcars$mpg, mtcars$wt)
cor.test(mtcars$mpg, mtcars$wt)
```

The *p-value* is very small and the correlation coefficient of `r cor(mtcars$mpg,mtcars$wt)` is very high. We also note that this value is negative, meaning that as the weight increases, the fuel efficiency decreases.  This makes intuitive sense.

Let's practice some data management before we look into these variables in more detail. 

Create a new data set called "my_mtcars", excluding the variables "vs" and "am" using vector notation. Then look at your new data set to make sure it worked.  We don't necessarily need to remove these variables to continue the analysis.  We are simply doing this so that you get more familiar with manipulating data frames.

```{r}
head(mtcars)
my_mtcars <- mtcars[, -c(8,9)] #Remove columns 8 and 9
head(my_mtcars, 10) 
```

Now, keeping the same name for your data set, exclude the "gear" and "carb" columns without using vector notation. Instead use the "subset" function. Check out the help (`?subset`) for this function to figure out how to exclude columns by name.  

```{r}
my_mtcars <- subset(my_mtcars, select = -c(gear, carb))
```

Note that the initial data of my_mycars with 9 variables no longer exists, because my syntax states to save the modified 7 variable data with the original name.

> QUESTION: How could you do this without overwriting the original data?

You should now have a data set called my_mtcars that has 32 observations of 7 variables. Check this.

```{r}
str(my_mtcars)
dim(my_mtcars)
```

> QUESTION: What does `## [1] 32 7` tells us?  Hint, recall vector notation.  

Another way of checking the number of rows and columns is using `nrow()` and `ncol()` functions.

The variable "cyl" represents the number of cylinders in the car engine. You'll see this is classified as a numeric variable. However, we aren't necessarily interested in this as an actual number. For us, the number of cylinders is more useful as a grouping mechanisms to serve as a factor, or categorical variable. Let's use the `as.factor` function to convert it, keeping the same variable name. Then check what the class of the variable is, to confirm the conversion worked. 

```{r}
my_mtcars$cyl <- as.factor(my_mtcars$cyl)
class(my_mtcars$cyl)
```

Creating a categorical factor variable will enable us to generate summary statistics and plot data by groups. 

We can now use this factor variable to group different operations. `tapply` is a great function to use for grouped operations. Check the help for `?tapply` and try to calculate the mean of "mpg" for each factor of the "cyl" variable.

```{r}
tapply(my_mtcars$mpg, my_mtcars$cyl, mean)
```

Now let's create box plots of our two variables of interest (mpg, wt) to visualize their distribution. First, change your graphic parameter to show two graphs side by side. 

> Question: How do you think you'd specify the title of each box plot?

```{r}
par(mfrow=c(1, 2))
boxplot(my_mtcars$mpg, main = "mpg")
boxplot(my_mtcars$wt, main = "weight")  
```

You can see that two points are potential outliers in the plot for`wt`. The plot gives you a tool to make a decision whether to remove the data points. Here we will keep them. 

Since `cyl` is a categorical variable, we can also visualize the distribution of `mpg` and `wt` across different cylinder classes.

```{r}
par(mfrow=c(1, 2))
boxplot(my_mtcars$mpg ~ my_mtcars$cyl, main = "mpg")
boxplot(my_mtcars$wt ~ my_mtcars$cyl, main = "weight") 
```

> QUESTION: Hhow do you modify the label for x axis and y axis?

Before we move forward, let's exclude these two observations by using a logical expression that removes the points (rows in the dataframe) in the data set where weight is greater than 5.3 tons. There are a few different ways to do this, as is the case with most things in **R**. Let's use the `subset` function again.

```{r}
my_mtcars <- subset(my_mtcars, wt <= 5.3)
```

You should now have a data set with 30 observations of 7 variables. 


## Data Table Manipulation with Dplyr

The most basic R skills is to query and manipulate various data tables. Table manipulation is also something that is almost always required,  regardless of what you decide to apply R for. For beginners, familiarizing and reinforcing table manipulation skills to meet different needs is a great way of improving R proficiency. If you wish to become really good at R, but don't know where to start, start with tables! 

The base R functions that come with the default R installation have the capacity for almost all the table manipulation needs (e.g., `split(), subset(), apply(), sapply(), lapply(), tapply(), aggregate()`). However, sometimes their syntax are less user-friendly and intuitive than some of the special packages built for table manipulation purposes. So, here we are introducing a few of the most useful table manipulation functions within `dplyr` package. Note that you will have to use `install.packages()` and `library()` function to download and activate the `dplyr` before using it. 

```{r, eval = T}
#install.packages("dplyr")
library(dplyr)
```

Read the csv data table named "panda_data.csv"
```{r}
panda_data<-read.csv(file="./data_IntroDataAnalysis/panda_data.csv")
```

### `select()` 
Select column(s) that meet specific pattern
```{r}
select(panda_data, panda_name) # select column called panda_name
select(panda_data, -panda_name) # select all columns in the data except name column
 select(panda_data, age: sex) # select a continuous block columns starting from age column and end on sex column 
select(panda_data, starts_with("genetic")) # select all columns that start with "genetic" in their column names
```
*  `starts_with` argument is very convenient because it allow you to select multiple columns that start with the same text. A few similar arguments are available to define other naming patterns.   
*  `ends_with()`= Select columns that end with a character string.  
*  `contains()`= Select columns that contain a character string.  
*  `matches()`= Select columns that match a regular expression.  
*  `one_of()`= Select columns names that are from a group of names.  
>Question 1: can you try to select all the columns contain "value" in their names?  
>Question 2: How do you select or exclude two columns: panda_name and age ?   


###  `filter()`  
Filter/select row(s) of data based on specific requirement of column(s) values\
```{r}
filter(panda_data, age %in% c(4,5))
filter (panda_data, age>5) # select rows that have age>5
filter(panda_data, age>5 | weight_kg>100) # select rows that have age>5 OR weight_kg >100 
filter (panda_data, age>5 & base =="CD") # select rows that have age>5 AND base column has CD has entry
filter(panda_data, age %in% c(4, 6)) # select rows that have age included a user defined list
```
>Question 1: How do you select rows with NA in genetic_value2 column?   
>Question 2: How do you select rows whose panda_name column are bao_bao or bei_bei ?  


###  pipe operator `%>%`   
This operator allows you to pipe the output from one function to the input of the next function. Instead of nesting functions (reading from the inside to the outside), the idea of of piping is to read the functions from left to right. It can also help you avoid creating and saving a lot of intermediate variables that you don't need to keep 
```{r}
pipe_result<- panda_data %>%
  select(panda_name, sex) %>%
  head()
pipe_result
```
> Question: Try to use pipe operator to 1. select all columns contain "genetic" in their names, 2. select the rows with genetic_value1  >80 AND genetic_value2 <90

### `arrange()` 
Arrange or re-order rows based on their value, the rows are arranged by default in ascending order  
```{r}
order_data1<- panda_data %>% 
	arrange(ID) 
order_data1

order_data2<- panda_data %>% 
	select(starts_with("genetic")) %>%
	arrange(genetic_value1, genetic_value2) %>%
	head
order_data2
# Now we learn pipe operator, can you understand what order_data1 and order_data2 are producing? 
```
> Question: Can you arrange the table first by age and then by panda weight in decending order?

###  `mutate()`   
Create new column(s) and define their values
```{r}
new_col<- panda_data %>%
	mutate(new_col = genetic_value1 - genetic_value2)  # before = is the name of the new column we are producing, and after = is how we want to give its value
new_col

new_col2<- panda_data %>%
	mutate(genetic_dif = genetic_value1 - genetic_value2, weight_g= weight_kg *1000) # you can create multiple columns at once 
new_col2
```
> Can you create a new column call zero and give it a value of 0 ?


###  `summarise()`   
Calculate summary statistics among all rows or rows within certain grouping, often used in combination with `group_by()` 
```{r}
sum_table <- panda_data%>% 
summarise (mean(weight_kg))
sum_table

sum_table2 <- panda_data%>% 
summarise (avg_wt= mean(weight_kg), min_wt= min(weight_kg))
sum_table2
```


### `group_by()`   
Divide data rows into groups based on grouping column(s) provided, often used in combination with other functions which define what you do with them after placing them in groups. When `group_by()` and `summarise()` are used together, you are essentially telling R to separate rows into different groups, and for each groups you use `summarise()` to generate a series of summary statistics that characterize the column values.    
```{r}
group_summary<- panda_data %>%
  group_by(base) %>%
  summarise(avg_wt= mean(weight_kg), 		min_wt= min(weight_kg))
group_summary
```

You can also create groups by the combination of two or multiple columns
```{r}
group_summary2<- panda_data %>%
  group_by(base, sex) %>%
  summarise(avg_wt= mean(weight_kg), 		min_wt= min(weight_kg))
group_summary2
```


###  Join table  
read in another table
```{r}
panda_data_med<-read.csv(file="./data_IntroDataAnalysis/panda_data_med.csv")
```
                          
*  `left_join()`   
Returns all records from the left table, and the matched (matched by shared ID columns) records from the right table. The result is NULL from the right side, if there is no match.
```{r}
left_join(panda_data, panda_data_med, by = c("year"= "year_vaccination"))

```

left join by multiple ID columns
```{r}
left_join(panda_data, panda_data_med, by = c("year" = "year_vaccination", "ID" = "ID"))
```

Other than `left_join()` there are a few other join functions that would join table differently. Sometimes you will find them useful for a specific table you want to make, but in general, you can reply on `left_join()` for most of the job.   
* `right_join()`: similar to left join, it keeps all records from the right table instead.  
* `inner_join()`: return only the matched records from both tables.  
* `full_join()`: return all records in both tables.  


### Optional: Convert between wide and long table

*  `gather()`  
Convert wide table to long table
```{r}
library(tidyr)
```

```{r}
to_longtable<- select(panda_data, panda_name, ID, contains("genetic")) %>%
  gather( key=genetic_number, value=genetic_value,  genetic_value1:genetic_value3)
head(to_longtable)
```

*  `spread()`   
Does the opposite, convert long table to wide table
```{r}
to_widetable<- spread(panda_data_med, vaccine_type, year_vaccination)
head(to_widetable)
```



## Linear Regression

Now let's move on to a linear regression analysis.

> QUESTION: What is the difference between correlation and regression?


### Model design and model fit

Model design involves deciding **what we are trying to explain** (i.e., the dependent variable) and **what we are going to use to explain it** (i.e., the independent variables).  These are questions that are informed by the researcher.

Then, we need to decide **what kind of model structure** is appropriate to the data set. In our case, we will start with a simple linear regression model. Later in the course, however, we will investigate more complex model structures.

Run a basic linear regression model, attempting to predict/explain the fuel efficiency of a car (mpg) based on its weight.  Finally, assign the model output "lm1". Remember to specify the `my_mtcars` data set in the model so the **R** understands that the name of dependent and independent variables are columns in that specific dataframe.

```{r}
lm1 <- lm(mpg ~ wt, data = my_mtcars)
```


### Investigate your model

Once you have fit the model, you can use the summary function to investigate the beta coefficient, SE, and intercept values. 

```{r}
summary(lm1)
```


### Model validation - Checking the assumptions of your model

IMPORTANT: Before you can trust your model, you need to examine whether the assumptions on which the model is based are actually valid. **If the assumptions are not satisfied, then your model will be unreliable**, at least to some extent!

If you apply the function **plot** to the model, **R** will provide a series of plots that will help you to inspect model assumptions. Click enter to get all the plots.

```{r}
plot(lm1)
```


1. **Assumption 1**: The residuals are normally distributed

Check the Q-Q (quantile-quantile) plot. A perfect normal distribution is represented by a straight line. Your residuals should be close to a straight line.

2. **Assumption 2**: The variances of the residuals are homogeneous (homoscedasticity)

The variability of the residuals should be uniform across the range of fitted values of the dependent variable. In the plot of **residuals vs fitted values of y** and **standardized residuals vs. fitted values** there should be a "scatter" of dots across the graphs, with no discernible pattern. Points should be randomly distributed.  If you have a scatter of points on one side of the graph, your data may NOT be homogeneous.

3. **Assumption 3**: The independent variables are independent of each other (no collinearity)

There are different ways you can address this before you fit your model. For instance, you can estimate the correlation of each pair of covariates and discard variables or exclude them from analysis if they highly correlated (positively or negatively). We will examine this later in the course when dealing with more complex models. 

4. **Assumption 4**: The data set does not contain serial auto-correlation

Serial autocorrelation is when there is significant correlation between successive data points in the data set. Spatial data or time-series data tend to be autocorrelated. There are different ways to deal with autocorrelation, such as using mixed-effect models (Note: mixed models are beyond the scope of this course). We will discuss some of these issues later.

5. **Assumption 5**: The model is not biased by unduly influential observations.

We can check this by looking at the plot of standardized residuals vs leverage and "Cook's Distance."

Leverage is a measure of the influence of individual data points on the model's parameters, measured on the scale of 0-1, where high values indicate a strong influence on the model's parameters.

Cook's distance is the sum of squared distances between the fitted values using the whole data set, and the fitted values with the *i*th observation removed. A large difference indicates that the *i*th observation exerts a strong influence on the model's parameters.

No values beyond 1.

***

> QUESTION: How does the QQ plot look?  Does it indicate a potential problem?

If your standardized residuals are not normally distributed you can **transform the dependent variable**.
Let's try a log transformation, which are commonly used:

```{r}
lm1 <- lm(log(mpg) ~ wt, data = my_mtcars)
plot(lm1)
```

> QUESTION: Does this improve the result?

The residuals now are randomly distributed and are homogeneous. We can now trust model results.

***

Continuing with interpreting results, the slope value should be `r lm1$coefficients[[2]]`. This means that for each unit of weight added to a car (1 ton), the log of miles per gallon it achieves is predicted to REDUCE by a value of `r abs(lm1$coefficients[[2]])`. There is a negative effect of weight on car efficiency.

The intercept of `r lm1$coefficients[[1]]` sets the start of the regression line at the weight of zero. In this case, this isn't very useful (a car will not weigh zero tons) but it is a necessary element of describing a linear relationship. Here, the equation for the line is `log(mpg) = 3.92593 -0.30555 (wt)`. Note that you can call the individual coefficients from a model directly using, in this example, "lm1$coefficients".

Now, plot again a scatterplot of weight vs. mpg and draw the regression line, in <span style="color:blue">blue</span>. First return your graphing parameter to it's default setting of (1,1)

```{r}
par(mfrow=c(1, 1))
plot(log(mpg) ~ wt, data = my_mtcars)
abline(lm1, col="blue")
```

Here are two other ways you can draw the predicted regression line:

```{r, eval = F}
abline(coef = lm1$coefficients, col="green")
abline(lm1$coefficients, col="red")
```


### Model Selection

In any mathematical modeling approach, there may be other variables, or some combination of variables, that are most effective and efficient at predicting your response variable of interest. In this case, our response variable is `mpg`. Looking at your plot of all the two-way relationships between variables, are there any other variables that may help predict `mpg`? Horsepower (`hp`) seems to bepotentially informative. The question now is, which model might be **best** at predicting `mpg`. 

Let's say we have three options: 
1. mpg ~ wt
2. mpg ~ wt + hp 
3. mpg ~ hp 

We've already fitted the first model.  Now, fit a linear regression model for the next two parameter combinations, giving them unique names (`lm2` and `lm3`), and look at the summary of their results.

```{r}
lm2 <- lm(log(mpg) ~ wt + hp, data = my_mtcars)
lm3 <- lm(log(mpg) ~ hp, data = my_mtcars)
summary(lm2)
summary(lm3)
```

As you'll see, all 3 of these models are reasonably good. Which is optimal? You might know that when you add more independent variables to a model, the model fit will often improve. But, it is certainly not ideal to have a model with a large number of independent variables (because we want to avoid overfitting). Akaike's Information Criteria (AIC) provides a good mechanism for model comparison. AIC ranks a model's performance by accounting for model fit while penalizing models with more variables. When adding a variable, the improved fit of the model must outweight the penalty, otherwise the improved fit will not be deemed worthwhile given the additional model complexity. 

Use the `AIC` function to compare the AIC values of our 3 models. The lowest AIC indicates a model is a "better" representation of the data.  Note, AIC is not a measure of fit. 

```{r}
AIC(lm1)
AIC(lm2)
AIC(lm3)
```

What can we conclude? The best model (from the three we have tried) for predicting miles per gallon of a car uses the horsepower and weight of the car in the prediction (`lm2`). 

To finish, let's see if we can now use this model (`lm2`) to predict the fuel efficiency of a car with a `hp = 225` and a `wt = 4.0` tons. Making prediction with a new set of independent variables when dependent variable is absent, is ultimately one of the main goals of a regression analyses. 

First, we need a dataframe with our new independent variables to be used in the prediction. Then, we can use the `predict()` function to apply our established linear model to the new information. Lastly, we need to transform miles per gallon back from the log scale to make it more easily interpretable. 

```{r}
nd <- data.frame(hp = 225, wt = 4.0)
exp(predict(lm2, newdata = nd, type = "response"))
```

We can get this same result without using the `predict` command, simply by writing out the linear equation. That is, our predicted response equals our intercept term, plus our coefficient for `wt` multiplied by the weight value, plus our coefficient for `hp` multiplied by our horsepower value (The function `predict()` just makes things a little easier for us). That is: 

$$y_i = \beta_0 + \beta_i x_i + \epsilon_i$$

We must also use the `exp()` function to back-transform from the log-scale.

```{r}
exp(lm2$coefficients[[1]] + (lm2$coefficients[[2]]*4.0) + (lm2$coefficients[[3]]*225))
```


## Analysis of Variance: ANOVA

The analysis of variance (ANOVA) is a lineal model, but the explanatory variable is categorical. As we saw before, the categorical variable must be a factor. Remember that if the category has been coded with numbers, **R** will assume the variable is continuous.

We will continue working with the `mtcars` data set. As we did before, we specified cylinders as a categorical variable:

```{r}
mtcars$cyl <- as.factor(mtcars$cyl)
class(mtcars$cyl)
```

### One-way ANOVA

Here we ask the question, do different cylinders imply more power? 

Our hypothesis ($H_1$) is that more cylinders will provide more power to the car, expressed in gross horsepower (HP). The null hypothesis ($H_0$) is that there is no difference in HP among number of cylinders.

To perform an ANOVA in **R**, we use the function `aov()` and `summary()` to see the results.

```{r}
model1 <- aov(hp ~ cyl, data=mtcars)
summary(model1)
```

In the results, the table shows us a *p-value*, representing the overall probability of significance for the categorical variable. If the term is significant, then we know at least one level is significantly different from the others.

We can reject the null hypothesis and conclude that gross horsepower differs among different number of cylinders.

However, before we know we can trust this result, we need to check the model assumptions. We will do this in the same way we did for the lineal model, using `plot()`

```{r}
plot(model1)
```

From the second plot we can conclude that the residuals are normally distributed (straight line).  From the first and third plot, we can see that the variances of the residuals are homogeneous (homoscedasticity). From the final plot, we can conclude that the model is not biased by unduly influential observations.

### Pairwise port-hoc tests

To understand how levels compare to each other once we know that the ANOVA is significant, we can use a post-hoc test. We will use the **Tukey post-hoc test** with the function `TukeyHSD()`.

```{r}
TukeyHSD(model1)
```

**diff** is the difference in mean between two levels, **lwr** and **upr** are the lower and upper 95% confidence intervals for the difference and **padj** is the `p-value` for the difference.

In this example, we can see that there is not HP significant difference between 6 and 4 cylinders  (`p > 0.05`), and that 8 cylinders have more HP than 4 and 6 cylinders (`p < 0.05`).


## Non-parametric Random Forest Model

In the past few decades, there has been an increasing number of studies that use non-parametric machine learning models for classification or regression. In the most simplistic of terms, non-parametric/machine learning models do not require the predictive variables to take a predetermined distribution (e.g. normal distribution). They are also relatively less stringent in terms of model assumptions.

One of the most popular and also most powerful machine learning tools is Random Forest. Random Forests is an ensemble method for classification- or regression-typed analyses that operate by constructing a series of decision trees. We will not go into detail to discuss the Random Forest algorithm or how it works. But in general, Random Forest models perform really well when there are:

1. Large number of independent variables.
2. Significant correlation among independent variables.
3. Independent variables that are not normally distributed.

### Random Forest Regression

Although the Random Forest algorithm is complicated, running a Random Forest model in **R** is as easy as running a linear model. For the most part, we just need to specify the data set, and the regression model formula. Let's use the "mtcars" data set again for this example.

Load the "mtcars"" data set, and load the random forest package.

```{r, eval= F}
install.packages("randomForest")
```

```{r}
data(mtcars)
library(randomForest)
```

Now let's run a model that includes all metrics in the data set to predict the `mpg` of a car.

Note that in the following code, the formula was simplified `mpg ~ .`. The `.`. represents the rest of the columns in the dataframe that are not defined as predictive variable. Whenever you have a dataframe that contains only the independent and dependent variables you need in the model, you can simplify the formula in this way. This essentially tells **R** to use all the variables included in the dataframe. 

```{r}
rf.model<-  randomForest(mpg ~ ., data=mtcars)
rf.model
```

Notice that one of the most important results of Random Forest is the "% variance explained". It can be considered equivalet to a $R^2$ value in a linear models quantifying how "good" the model fits the data. The closer the "% var explained" is to 100, the better the model's performance. 

Here you can see the default number of trees used in the model is 500. In reality, the number of tree is one of very few parameters that need to be modified by the user. The number of trees determines the number of ensemble models that are created in the modeling process. As we increase the tree number, the model results will tend to vary and eventually stabilize. But we don't want to create too many ensemble tree models because it will take more computational time. The general rule is to increase the number of trees incrementally until the value of "% var explained" changes very little.

Let's change the default number of trees parameter to 1000 and 5000 and observe the difference in model results

```{r}
rf.model.1000<- randomForest( mpg ~ ., data=mtcars, ntree=1000)
rf.model.1000
rf.model.5000<- randomForest( mpg ~ ., data=mtcars, ntree=5000)
rf.model.5000
```

The bigger number of trees (`ntree`) does not change the "% var explained" significantly. So the default `ntree= 500` is as good as `ntree= 1000` or `ntree=5000`.

Lastly, a convenient feature of Random Forest model is that it internally summarizes how important each variable is to model performance. The model does this by replacing each predictive variable separately with randomly permuted values. After running the model with replacement, the more the model accuracy decreases, the more important the specific variable (that was replaced in the process) is.  You can retrieve and visualize the variable importance data using `importance()` and `varImPlot()` functions. 

```{r}
importance(rf.model)        
varImpPlot(rf.model)  
```

Note that "IncNodePurity" is just a measure of how much the model performance has decreased. The higher that value, the more important the variable is. 

> QUESTION: Which variable is contributing most to predicting mpg of a car? 

Lastly, just as with all the other models, the most powerful use of Random Forest model is to predict the dependent variable when you have a set of independent variables. You can, once again, use the `predict()` function to perform this action. Below, we will create a fictious car, changing the Hornet-4 Drive from 6 cylinder to 8 cylinders. We want to know what the mpg of this new car is. 

```{r}
new.car<- mtcars[4,-1]
new.car$cyl<- 8
new.car
predict(rf.model, newdata=new.car) 
```

> QUESTIONS: 
>
> 1) Do you think the prediction make sense? Why?
> 2) Can you build a random forest classification model to predict the number of cylinders based on other metrics included in the dataset? 

